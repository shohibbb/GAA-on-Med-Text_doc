.. _result:

Evaluation Results
==================

This section summarizes the main findings from the experiments. Full results, including detailed tables and logs, are available in the ``results/`` folder.

Model Results
-------------

Traditional machine learning models—including Logistic Regression (LR), SVM, Naive Bayes, Random Forest (RF), and XGBoost—were compared with a Stack Classifier and MedBERT. Across different word representations (Word2Vec, GloVe, and FastText), RF consistently achieved the best accuracy and robustness, with only minimal performance drops across representations.

The Stack Classifier showed mixed results: it performed slightly worse with Word2Vec and GloVe, but remained strong with FastText. MedBERT, although designed for medical text, did not surpass the performance of classical models—likely because most models already achieved near-perfect accuracy.

RF and the Stack Classifier were selected as victim models for the adversarial attack phase due to their strong baseline performance.

Attack Results
--------------

Two attack algorithms (TextFooler and a modified version using WordSwapMaskedLM) were applied. Results showed that:

- **MedBERT** was the most vulnerable, with accuracy dropping drastically after attacks and a high attack success rate (>80% in some cases).
- **Traditional ML models** and the **Stack Classifier** were more resilient, with lower success rates (often <10%).
- The modified TextFooler generally reduced the effectiveness of attacks, highlighting a trade-off between attack success and text quality.

These findings indicate that while transformer-based models such as MedBERT are powerful in normal classification tasks, they remain highly sensitive to adversarial perturbations.

Perturbed Text Analysis
-----------------------

The adversarial examples generated by TextFooler and its variant often maintained high similarity with the original inputs (≈70–83%), showing that small word substitutions were enough to mislead models.

Successful attacks typically involved minor synonym replacements, though sometimes with small grammatical errors (e.g., “a alot weight”). Failed attacks occurred when text changes were too minimal or when the algorithm could not find suitable substitutions. Skipped cases happened when the model had already misclassified the original input.

Overall, the analysis shows that perturbations with high similarity to the original text can still significantly degrade model accuracy, especially in transformer-based architectures.